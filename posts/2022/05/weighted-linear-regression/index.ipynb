{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Weighted Linear Regression'\n",
    "author: \"Andrea Conti\"\n",
    "tags: []\n",
    "date: '2022-05-09T16:18:28+02:00'\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Linear Regression\n",
    "\n",
    "If you are here there are high chances you already know how a simple linear regression works, it is the first and simplest algorithm you meet you your machine learning journey, but let's recap since it will be useful to later introduce its _weighted_ form.\n",
    "\n",
    "Let's say that you have a set of values $X$ and for each of them a _target_ value $Y$, if you plot them can be easily seen that they could be approximated by a simple straight line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0, 5, 1000)\n",
    "y = 3 * x + 3 + 2 * np.random.randn(1000)\n",
    "\n",
    "plt.plot(x, y, \"b.\", alpha=0.4)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to find the best straight line which better approximates the data behaviour, this means that you want to find $\\alpha$ and $\\beta$ such that:\n",
    "\n",
    "$$\\operatorname{argmin}_{\\alpha, \\beta} \\sum_i (y_i - (\\alpha + \\beta x_i)^2$$\n",
    "\n",
    "Once you have them you can predict the output for every $X$ with:\n",
    "\n",
    "$$y_{pred} = \\alpha + \\beta x$$\n",
    "\n",
    "The really interesing thing about linear regression is that the best fit can be optained in _closed form_, i.e. solving a simple formula:\n",
    "\n",
    "$$\\beta = \\frac{\\sum_i(x_i - \\hat x)(y_i - \\hat y)}{\\sum_i(x_i - \\hat x)}$$\n",
    "$$\\alpha = \\hat y - \\beta \\hat x$$\n",
    "\n",
    "Where $\\hat x$ and $\\hat y$ are respectively the mean of all the $X$ and $Y$ values. It could be expressed also by means of matrix multiplications but here we'll stick with this form since it can be easily implemented through broadcasting in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(x: np.ndarray, y: np.ndarray):\n",
    "    xm, ym = np.mean(x), np.mean(y)\n",
    "    b = np.sum((x - xm) * (y - ym)) / np.sum(np.square(x - xm))\n",
    "    a = ym - b * xm\n",
    "    return a, b\n",
    "\n",
    "a, b = linear_regression(x, y)\n",
    "plt.plot(x, y, \"b.\", alpha=0.4)\n",
    "plt.plot(x, a + b * x, \"r--\", linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's important to observe that linear regression makes some assumptions over the data used, if these assumptions do not hold the closed form will lead to suboptimal results.\n",
    "\n",
    "### Linearity\n",
    "The dependent variable ($Y$) has a linear relationship with the independent one ($X$), this is pretty obvious, for instance the following function can only poorly represented through a linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "y2 = 5 + 2 * x + 4 * x**2\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.plot(x, y2, \"b-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality\n",
    "The observation errors are normally distributed\n",
    "\n",
    "### Independency\n",
    "The error of each observation is independent to the others, this means that the error that you can see in a observation can not affect the error of other observations\n",
    "\n",
    "### Low Multi-Collinearity\n",
    "There isn't high correlation among the independent variables \n",
    "\n",
    "### Homoscedasticity\n",
    "All the observation errors have the same finite variance, basically this means that you assume that the noise present in you observations varies always inside the same range following a gaussian distribution which has the same variance. When this assumption does not hold you are dealing with a case of _Heteroscedasticity_. Following an example of Homoscedasticity in which the linear regression fails due to the high quantity of _outliers_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hetero = 3 * x + 3 + np.abs(5 * x * np.sin(x) * np.random.randn(1000))\n",
    "a, b = linear_regression(x, y_hetero)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.xlabel(\"X\"); plt.ylabel(\"Y\")\n",
    "plt.plot(x, y_hetero, \"bo\", alpha=0.3)\n",
    "plt.plot(x, a + x * b, \"r--\", linewidth=3.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Linear Regression\n",
    "\n",
    "We are particularly interested in the Homoscedasticity property since relaxing this linear regression property leads to the capability to handle outliers. We can model each observation to be a random normal variable with its own variance and we want to _weight_ each observation by means of its variance. The problem to be solved becomes thus:\n",
    "\n",
    "$$\\operatorname{argmin}_{\\alpha, \\beta} \\sum_i \\frac{1}{\\sigma_i}(y_i - (\\alpha + \\beta x_i)^2$$\n",
    "\n",
    "The closed form to find $\\alpha$ and $\\beta$ in the weighted case is:\n",
    "\n",
    "$$\\beta = \\frac{\\sum_i w_i(x_i - \\hat x)(y_i - \\hat y)}{\\sum_i w_i (x_i - \\hat x)^2}$$\n",
    "$$\\alpha = \\hat y - \\beta \\hat x$$\n",
    "$$\\hat x = \\frac{\\sum_i w_ix_i}{\\sum_i w_i}$$\n",
    "$$\\hat y = \\frac{\\sum_i w_iy_i}{\\sum_i w_i}$$\n",
    "\n",
    "Below an example where a portion of data is not representative and it is ignored assigning them zero weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_linear_regression(x: np.ndarray, y: np.ndarray, w: np.ndarray):\n",
    "    xm = np.sum(w * x) / np.sum(w)\n",
    "    ym = np.sum(w * y) / np.sum(w)\n",
    "    b = np.sum(w * (x - xm) * (y - ym)) / np.sum(w * np.square(x - xm))\n",
    "    a = ym - b * xm\n",
    "    return a, b\n",
    "\n",
    "y_outliers = 3 * x + 3 * 10 * np.random.randn(1000)\n",
    "y_outliers[-200:] = y_outliers[-200:] + 80\n",
    "\n",
    "plt.plot(x, y_outliers, \"b.\", alpha=0.4)\n",
    "for i in [0.0, 1.0]:\n",
    "    weights = np.ones_like(y_outliers)\n",
    "    weights[-200:] = i\n",
    "    a, b = weighted_linear_regression(x, y_outliers, weights)\n",
    "    plt.plot(x, a + b * x, label=f\"weight: {i:.2f}\", linewidth=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "abcf3100464b9e7aa187ce16046b64eb0debc60cc49334d7b969683768e90a81"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
