<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>Range-Agnostic Multi-View Depth Estimation With Keyframe Selection - Andrea Conti PhD</title><meta name="Description" content="Methods for 3D reconstruction from posed frames require prior knowledge about the scene metric range, usually to recover matching cues along the epipolar lines and narrow the search range. However, such prior might not be directly available or estimated inaccurately in real scenarios – e.g., outdoor 3D reconstruction from video sequences – therefore heavily hampering performance. In this paper, we focus on multi-view depth estimation without requiring prior knowledge about the metric range of the scene by proposing an efficient and purely 2D framework that reverses the depth estimation and matching steps order. Moreover, we demonstrate the capability of our framework to provide rich insights about the quality of the views used for prediction. We achieve state-of-the-art performance on Blended and TartanAir, two challenging benchmarks featuring posed video frames in various scenarios, and demonstrate generalization capabilities and stereo perception applicability on UnrealStereo4K. Finally, we show that our framework is accurate in controlled environments with fixed depth ranges, such as those featured in the DTU dataset."><meta property="og:title" content="Range-Agnostic Multi-View Depth Estimation With Keyframe Selection" />
<meta property="og:description" content="Methods for 3D reconstruction from posed frames require prior knowledge about the scene metric range, usually to recover matching cues along the epipolar lines and narrow the search range. However, such prior might not be directly available or estimated inaccurately in real scenarios – e.g., outdoor 3D reconstruction from video sequences – therefore heavily hampering performance. In this paper, we focus on multi-view depth estimation without requiring prior knowledge about the metric range of the scene by proposing an efficient and purely 2D framework that reverses the depth estimation and matching steps order. Moreover, we demonstrate the capability of our framework to provide rich insights about the quality of the views used for prediction. We achieve state-of-the-art performance on Blended and TartanAir, two challenging benchmarks featuring posed video frames in various scenarios, and demonstrate generalization capabilities and stereo perception applicability on UnrealStereo4K. Finally, we show that our framework is accurate in controlled environments with fixed depth ranges, such as those featured in the DTU dataset." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://andreaconti.github.io/projects/range_agnostic_multi_view_depth/" /><meta property="article:section" content="projects" />
<meta property="article:published_time" content="2024-03-18T17:32:00+02:00" />
<meta property="article:modified_time" content="2024-03-18T17:32:00+02:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Range-Agnostic Multi-View Depth Estimation With Keyframe Selection"/>
<meta name="twitter:description" content="Methods for 3D reconstruction from posed frames require prior knowledge about the scene metric range, usually to recover matching cues along the epipolar lines and narrow the search range. However, such prior might not be directly available or estimated inaccurately in real scenarios – e.g., outdoor 3D reconstruction from video sequences – therefore heavily hampering performance. In this paper, we focus on multi-view depth estimation without requiring prior knowledge about the metric range of the scene by proposing an efficient and purely 2D framework that reverses the depth estimation and matching steps order. Moreover, we demonstrate the capability of our framework to provide rich insights about the quality of the views used for prediction. We achieve state-of-the-art performance on Blended and TartanAir, two challenging benchmarks featuring posed video frames in various scenarios, and demonstrate generalization capabilities and stereo perception applicability on UnrealStereo4K. Finally, we show that our framework is accurate in controlled environments with fixed depth ranges, such as those featured in the DTU dataset."/>
<meta name="application-name" content="Andrea Conti PhD">
<meta name="apple-mobile-web-app-title" content="Andrea Conti PhD"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://andreaconti.github.io/projects/range_agnostic_multi_view_depth/" /><link rel="prev" href="https://andreaconti.github.io/projects/revisiting_depth_completion_from_stereo_matching/" /><link rel="next" href="https://andreaconti.github.io/projects/depth_on_demand/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Range-Agnostic Multi-View Depth Estimation With Keyframe Selection",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/andreaconti.github.io\/projects\/range_agnostic_multi_view_depth\/"
        },"genre": "projects","keywords": "multi-view, depth-completion, deep-learning, computer-vision, 3dv","wordcount":  474 ,
        "url": "https:\/\/andreaconti.github.io\/projects\/range_agnostic_multi_view_depth\/","datePublished": "2024-03-18T17:32:00+02:00","dateModified": "2024-03-18T17:32:00+02:00","publisher": {
            "@type": "Organization",
            "name": "Andrea Conti"},"author": {
                "@type": "Person",
                "name": "Andrea Conti"
            },"description": "Methods for 3D reconstruction from posed frames require prior knowledge about the scene metric range, usually to recover matching cues along the epipolar lines and narrow the search range. However, such prior might not be directly available or estimated inaccurately in real scenarios – e.g., outdoor 3D reconstruction from video sequences – therefore heavily hampering performance. In this paper, we focus on multi-view depth estimation without requiring prior knowledge about the metric range of the scene by proposing an efficient and purely 2D framework that reverses the depth estimation and matching steps order. Moreover, we demonstrate the capability of our framework to provide rich insights about the quality of the views used for prediction. We achieve state-of-the-art performance on Blended and TartanAir, two challenging benchmarks featuring posed video frames in various scenarios, and demonstrate generalization capabilities and stereo perception applicability on UnrealStereo4K. Finally, we show that our framework is accurate in controlled environments with fixed depth ranges, such as those featured in the DTU dataset."
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('light' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'light' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Andrea Conti PhD">Andrea Conti</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/projects/"> Projects </a><a class="menu-item" href="/about"> About </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Andrea Conti PhD">Andrea Conti</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/projects/" title="">Projects</a><a class="menu-item" href="/about" title="">About</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX" style="text-align: center; font-size: 200%;">Range-Agnostic Multi-View Depth Estimation With Keyframe Selection</h1><div style="display: none;" class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>Andrea Conti</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2024-03-18">2024-03-18</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;474 words&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;3 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#method">Method</a></li>
    <li><a href="#qualitatives">Qualitatives</a>
      <ul>
        <li><a href="#blended">Blended</a></li>
        <li><a href="#tartanair">TartanAir</a></li>
        <li><a href="#unrealstereo4k">UnrealStereo4K</a></li>
        <li><a href="#dtu">DTU</a></li>
      </ul>
    </li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><!-- raw HTML omitted -->


<p style="text-align: center;">
    <a href="https://andreaconti.github.io" target="_blank">Andrea Conti</a>
    &middot 
    <a href="https://mattpoggi.github.io" target="_blank">Matteo Poggi</a>
    &middot
    <a href="https://scholar.google.com/citations?user=yYHJGZUAAAAJ" target="_blank">Valerio Cambareri</a>
    &middot
    <a href="http://www.vision.deis.unibo.it/~smatt/Site/Home.html" target="_blank">Stefano Mattoccia</a>
</p>
<p style="text-align: center">
    <a href="https://arxiv.org/pdf/2401.14401.pdf" target="_blank">[Paper]</a>
    <a href="https://github.com/andreaconti/ramdepth.git" target="_blank">[Code]</a>
    <a href="https://github.com/andreaconti/ramdepth/blob/main/evaluate.ipynb" target="_blank">[Demo]</a>
</p>

<h2 id="overview">Overview</h2>
<p>Multi-View 3D reconstruction techniques process a set of source views and a reference view to yield an estimated depth map for the latter. Unluckily, state-of-the-art frameworks</p>
<ol>
<li>require to know <em>a priori</em> the depth range of the scene, in order to sample a set of <em>depth hypotheses</em> and build a meaningful <em>cost volume</em>.</li>
<li>do not take into account the <em>keyframes selection</em>.</li>
</ol>
<p>In this paper, we propose a novel framework <strong>free from prior knowledge of the scene depth range</strong> and capable of <strong>distinguishing the most meaningful source frames</strong>. The proposed method unlocks the capability to apply multi-view depth estimation to a wider range of scenarios like large-scale outdoor environments, top-view buildings and large-scale outdoor environments.</p>
<h2 id="method">Method</h2>
<p>Our method relies on an <strong>iterative approach</strong>: starting from a zero-initialized depth map we extract geometrical correlation cues and update the prediction. At each iteration we feed also information extracted from the reference view only (the one on which we desire to compute depth).
Moreover, at each iteration we use a different source view to exploit multi-view information in a round-robin fashion. For more details please refer to the <a href="" rel="">paper</a>.</p>



<style>
    .imggridbox {
       margin: 0px 0px;
       text-align: center;
       font-weight: lighter;
       font-size: smaller;
    }

    .imggrid-row {
        margin: 0px 0px;
        display: flex;
        justify-content: center;
    }

    .imggrid-col {
        margin: 0px 0px;
        display: flex;
        justify-content: center;
        flex-direction: column;
    }

    .caption {
        text-align: justify;
        font-weight: bolder;
        margin: 0px auto;
    }
</style>




<div class="imggridbox">
    <div class="imggrid-row">
<figure><img src="static/model.png" width="800px"/>
</figure>

</div>
    
        <div class="caption"><b>Framework Description.</b> Our model instantiates an initial depth map and builds a pair-wise correlation table for each source image. Then, deformable sampling is iteratively performed over it, and the depth state is updated accordingly. Final depth prediction is upsampled through convex upsampling.</div>
    
</div>
<h2 id="qualitatives">Qualitatives</h2>
<p>We provide a wide set of qualitative results from different scenarios, since our approach can easily generalize not being tied to a specific depth range known <em>a priori</em>.</p>
<h3 id="blended">Blended</h3>
<p>The <a href="" rel="">Blended</a> dataset is a large dataset providing ground-truth data for wide aerial scenes and short range objects. Our framework provides really accurate predictions without knowing the scene depth range at all.</p>
<figure><img src="static/blended_example.jpg" width="800px"/>
</figure>

<h3 id="tartanair">TartanAir</h3>
<p>The <a href="" rel="">TartanAir</a> Dataset is a large synthetic dataset symulating the flight of a drone in a wild scenarios. Moreover, the in the same scene the depth range of each view may be extremelly various. We report a set of examples from indoor and underwater scenes.</p>
<figure><img src="static/tartanair_example.jpg" width="800px"/>
</figure>

<h3 id="unrealstereo4k">UnrealStereo4K</h3>
<p>The capability of our framework to not being tied to a specific depth range allows the deployment in stereo scenarios without any other information required other than the rectified stereo frames and the baseline information. To assess this capability we test also on the <a href="" rel="">UnrealStereo4K</a> Dataset.</p>
<figure><img src="static/unrealstereo4k_example.jpg" width="800px"/>
</figure>

<h3 id="dtu">DTU</h3>
<p>Finally, we test also on <a href="" rel="">DTU</a>, one of the most commonly used dataset in the Multi-View Stereo scenario. This dataset is characterized by a set of small objects framed from multiple views by means of a robotic arm.</p>
<figure><img src="static/dtu.jpg" width="800px"/>
</figure>

<h2 id="reference">Reference</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bibtex" data-lang="bibtex"><span class="line"><span class="cl"><span class="nc">@InProceedings</span><span class="p">{</span><span class="nl">Conti_2024_3DV</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="na">author</span>    <span class="p">=</span> <span class="s">{Conti, Andrea and Poggi, Matteo and Cambareri, Valerio and Mattoccia, Stefano}</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="na">title</span>     <span class="p">=</span> <span class="s">{Range-Agnostic Multi-View Depth Estimation With Keyframe Selection}</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on 3D Vision}</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="na">month</span>     <span class="p">=</span> <span class="s">{March}</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="na">year</span>      <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c">}</span>
</span></span></code></pre></div></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2024-03-18</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/multi-view/">multi-view</a>,&nbsp;<a href="/tags/depth-completion/">depth-completion</a>,&nbsp;<a href="/tags/deep-learning/">deep-learning</a>,&nbsp;<a href="/tags/computer-vision/">computer-vision</a>,&nbsp;<a href="/tags/3dv/">3dv</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/projects/revisiting_depth_completion_from_stereo_matching/" class="prev" rel="prev" title="Revisiting Depth Completion from a Stereo Matching Perspective for Cross-Domain Generalization"><i class="fas fa-angle-left fa-fw"></i>Revisiting Depth Completion from a Stereo Matching Perspective for Cross-Domain Generalization</a>
            <a href="/projects/depth_on_demand/" class="next" rel="next" title="Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor">Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
<div id="comments"><div id="disqus_thread" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://disqus.com/?ref_noscript">Disqus</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2021 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">Andrea Conti</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><script type="text/javascript" src="https://%3cnil%3e.disqus.com/embed.js" defer></script><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":null,"maxResultLength":null,"noResultsFound":"No results found","snippetLength":null}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
